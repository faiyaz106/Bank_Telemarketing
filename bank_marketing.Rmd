---
title: "Bank Telemarketing (STAT 530)"
author: "Faiyaz Ahmad, Kruthika Gopinathan, Rutuja Magdum"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---
# 1. Loading Data and  Important Library
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
data<-read.csv('Bank-Full.csv',sep=';')
# Essential Libraries
library(ggplot2)
library(gridExtra)
library(rpart)
library(rpart.plot)
library(pROC)
```

# 2. Some Important functions( For Model Evaluation)

### 2.1: Logistic Regression Model Evaluation (Precsion, Recall, and F1)
```{r}
# Model evaluation for logistic regression 
model_evaluation <- function(model,data,threshold){
  prob<-stats::predict(model, newdata=data, type="response")
  pred<- rep(0,dim(data)[1]) #createazerovector
  pred[prob>threshold]=1
  tab<-table(pred,data$y)
  tp<-tab[4]
  fp<-tab[2]
  tn<-tab[1]
  fn<-tab[3]
  Accuracy<-(tp+tn)/sum(tab)
  Precision<-(tp/(tp+fp))
  Recall<-(tp/(tp+fn))
  F_1<-2*Precision*Recall/(Precision+Recall)
  x<-c(Accuracy,Precision,Recall,F_1)
  return(x)
  }
```

### 2.2 Function for Drawing ROC_AUC_Curve for Logistic Regression
```{r}
roc_auc_curve<-function(model,data){
  prob1<-stats::predict(model, newdata=data, type="response")
  rocobj <- roc(test$y, prob1)
  auc <- round(auc(test$y, prob1),4)
  #create ROC plot
  ggroc(rocobj, colour = 'steelblue', size = 2) +
    ggtitle(paste0('ROC Curve ', '(AUC = ', auc, ')'))
}

```

### 2.3. Model Evaluation Report for Logistic Regression 
```{r data}
model_report<-function(model,train,test,threshold){
  test_set<-model_evaluation(model,test,threshold)
  test_set
  training_set<-model_evaluation(model,train,threshold)
  row.names<-c("Accuracy","Precision","Recall","F_1")
  df1<-data.frame(row.names,training_set,test_set)
  return(df1)
}

```

### 2.4. Decision Tree Model Evaluation function 

```{r}
decision_tree_eval <- function(model,data){
  y_pred<-predict(model,data,type='class')
  tab<-table(y_pred,data$y)
  tp<-tab[4]
  fp<-tab[2]
  tn<-tab[1]
  fn<-tab[3]
  Accuracy<-(tp+tn)/sum(tab)
  Precision<-(tp/(tp+fp))
  Recall<-(tp/(tp+fn))
  F_1<-2*Precision*Recall/(Precision+Recall)
  x<-c(Accuracy,Precision,Recall,F_1)
  return(x)
  }

decision_tree_report<-function(model,train,test){
  test_set<-decision_tree_eval(model,test)
  training_set<-decision_tree_eval(model,train)
  row.names<-c("Accuracy","Precision","Recall","F_1")
  df1<-data.frame(row.names,training_set,test_set)
  return(df1)
}


```

# 3. Data Preprocessing


### 3.1 Converting the data categorical data as factor
```{r}
data$y<-as.factor(data$y)
data$job<-as.factor(data$job)
data$marital<-as.factor(data$marital)
data$education<-as.factor(data$education)
data$default<-as.factor(data$default)
data$housing<-as.factor(data$housing)
data$loan<-as.factor(data$loan)
data$month<-as.factor(data$month)
data$poutcome<-as.factor(data$poutcome)

```

### 3.2. Splitting the data into Training(80%) and Test(20%) set

```{r}
#make this example reproducible
set.seed(1)
#use 80% of data set as training set and 20% as test set
sample <- sample(c(TRUE, FALSE), nrow(data),replace=TRUE,prob=c(0.8,0.2))

train <- data[sample, ]
test  <- data[!sample, ]

```


### 3.3. Treating outliers
     1. For 'balance' there are lot of outliers.
     2. As data is divided into training and test.
     3. Process of treating outliers:
        IQR (Inter-quartile range): Q3-Q1
        If numerical feature is not in between Q1-1.5IQR to Q3+1.15IQR, then that particular data point will be imputed with median.
     4. We treating the outliers in training set, and in testing set, same value need to imputed as of training set.

#### 3.3.1 Let treat the outliers related to 'balance' feature
```{r}

attach(train)
par(mfrow=c(1,2))
hist(balance)
hist(log(balance))
```

```{r}
ggplot(train, aes(balance,y))+
  geom_point()+
  geom_boxplot()

```


```{r}
median<-median(train$balance)
q1<-quantile(train$balance,probs = c(.25, .5, .75))
IQR<-q1[3]-q1[1]
Max<-q1[3]+1.5*IQR
Min<-q1[1]-1.5*IQR
train$balance[train$balance>Max]<-median
train$balance[train$balance<Min]<-median
test$balance[test$balance>Max]<-median
test$balance[test$balance<Min]<-median

```

### 3.3.2. Treating the outliers for 'Duration' feature
```{r}
median<-median(train$duration)
q1<-quantile(train$duration,probs = c(.25, .5, .75))
IQR<-q1[3]-q1[1]
Max<-q1[3]+1.5*IQR
Min<-q1[1]-1.5*IQR
train$duration[train$duration>Max]<-median
train$duration[train$duration<Min]<-median
test$duration[test$duration>Max]<-median
test$duration[test$duration<Min]<-median

```

### 3.3.3 Treating the outliers for 'Campaign' feature
```{r}
median<-median(train$campaign)
q1<-quantile(train$campaign,probs = c(.25, .5, .75))
IQR<-q1[3]-q1[1]
Max<-q1[3]+1.5*IQR
Min<-q1[1]-1.5*IQR
train$campaign[train$campaign>Max]<-median
train$campaign[train$campaign<Min]<-median
test$campaign[test$campaign>Max]<-median
test$campaign[test$campaign<Min]<-median

```

# 4. Feature Selection: 

### 4.1 Categorical Features

###  Test of Independence: For categorical Features
     
     Null Hypothesis: There is no association between two categorical features.
     Alternate Hypothesis: There is assocation between two features.
     
     
#### 4.1.1. Credit Default vs y
```{r}
tab<-with(train,table(default,y))
addmargins(prop.table(tab))
chisq.test(tab)
```
#### Comment: 
     
     1. P-value is less than the significance level of 0.05, we can reject the null hypothesis.
     2. Credit default is associated with y
     

#### 4.1.2. personal loan vs y
```{r}
tab<-with(train,table(loan,y))
addmargins(prop.table(tab))
chisq.test(tab)
```

#### Comment: 
     
     1. P-value is less than the significance level of 0.05, we can reject the null hypothesis.
     2. Personal loan is associated with y
     
     
#### 4.1.3. housing loan vs y
```{r}
tab<-with(train,table(housing,y))
addmargins(prop.table(tab))
chisq.test(tab)
```
#### Comment: 
     
     1. P-value is less than the significance level of 0.05, we can reject the null hypothesis.
     2. Housing loan is associated with y
     
#### 4.1.5. education vs y
```{r}
tab<-with(train,table(education,y))
addmargins(prop.table(tab))
chisq.test(tab)
```

#### 4.1.5. job vs y
```{r}
tab<-with(train,table(job,y))
addmargins(prop.table(tab))
chisq.test(tab)
```

#### 4.1.6. month vs y
```{r}
tab<-with(train,table(month,y))
addmargins(prop.table(tab))
chisq.test(tab)
```





#### 4.1.7. Credit Default vs Personal Loan
```{r}
tab<-with(train,table(default,loan))
addmargins(prop.table(tab))
chisq.test(tab)
```

#### Comment: 
     
     1. P-value is less than the significance level of 0.05, we can reject the null hypothesis.
     2. Credit default dependent on Loan.
     
#### 4.4.8. Housing Loan vs Personal Loan
```{r}
tab<-with(data,table(housing,loan))
addmargins(prop.table(tab))
chisq.test(tab)
```

#### Comment: 
     
     1. Null Hypothesis: Housing Loan not associated with Personal Loan .
        Alternate Hypothesis: Housing Loan is associated with  personal loan.
     
     2. P-value is less than the significance level of 0.05, we can reject the null hypothesis 
     
     2. Housing loan is associated with personal loan.
     
     
#### 4.1.9. Education vs Job
```{r}
tab<-with(train,table(education,job))
addmargins(prop.table(tab))
chisq.test(tab)
```

#### Comment: 
     
     1. Null Hypothesis: Education not associated with job type.
        Alternate Hypothesis: Education is associated with  job type.
     
     2. P-value is less than the significance level of 0.05, we can reject the null hypothesis 
     
     3. Education is associated with the Job type
     


## 4.1.10 Does adding education along with job as feature in model will result in better model than alone with job type?
      
      i) We will first fit the model with job and do anova test on it.
      ii) Then add education varible in first model to check the impact of new variable on model. To perform statistically signficance, we will perform anova test between two mode.

## 4.1.4 Does adding education along with housing along with loan as feature in model will result in better model than alone with housing type?  
```{r}
mod1<-glm(y~housing,family='binomial',data=train)
mod2<-glm(y~housing+loan,family='binomial',data=train)
anova(mod1,mod2,test='Chisq')
```

## Comment: 
    
    1.From chisquare test,it is evident that housing and loan is associated with each other.
    2. From above anova test, it suggest that adding the housing along with loan will give significant impact. As p_value is less than 0.05 significance level.
    3. So, we can use housing along with the loan.


```{r}
mod1<-glm(y~loan,family='binomial',data=train)
mod2<-glm(y~loan+default,family='binomial',data=train)
anova(mod1,mod2,test='Chisq')
```
## Comment: 
    
    1. From above anova test, it suggest adding the loan along with the credit default have significant effect. As p_value is less than 0.05 significance level.


```{r}
mod1<-glm(y~loan,family='binomial',data=train)
mod2<-glm(y~loan+housing,family='binomial',data=train)
anova(mod1,mod2,test='Chisq')
```
    1. From above, it is clear, that housing and loan are completely associated with each other. Can act as single variable.
    

```{r}
mod1<-glm(y~month,family='binomial',data=train)
anova(mod1,test='Chisq')
```


### let's check the distributin of 'y' in training and testing set.
```{r}
# Distribution of 'y' in training set
tab<-table(train$y)
prop.table(tab)

# Distribution of 'y' in test set
tab<-table(test$y)
prop.table(tab)
```

## Train the model.

### 1. Demography: Age,job,marital
```{r}
dem_mod<-glm(y~age+job+marital,family='binomial',data=train)
summary(dem_mod)
```




 
```{r}   
anova(dem_mod,test='Chisq')

```

```{r}
threshold=0.1
model_report(dem_mod,train,test,threshold)

```

```{r}
roc_auc_curve(dem_mod,test)
```  

### 2.Financial Characteristics: Balance, housing,loan


```{r}
fin_mod<-glm(y~balance+housing+loan,family='binomial',data=train)
summary(fin_mod)
```
```{r}   
anova(fin_mod,test='Chisq')

```



```{r}
threshold=0.1
model_report(fin_mod,train,test,threshold)

```


## Comment: 
       
       1. Precision and Recall is very less on positive cases (i.e Response variable: 'y')
       2. From here, It can be drawin, that Financial characteristics are not good predictor of subscription rate.
       3. Precision and Recall for positive class is zero for test set.


```{r}
roc_auc_curve(fin_mod,test)

```






### 3. Campaign: duration,campaign,day,month

```{r}
camp_mod<-glm(y~duration+campaign+day+month,'binomial',data=train)
summary(camp_mod)

```

```{r}   
anova(camp_mod,test='Chisq')

```

```{r}
threshold=0.1
model_report(camp_mod,train,test,threshold)

```


```{r}
options(repr.plot.width=6, repr.plot.height=5)

dt_camp<-rpart(y~duration+campaign+day+month,cp=0.001,maxdepth=7, minbucket=5,method='class',data=train)
prp(dt_camp)  #,space=4,split.cex=1.5, nn.border.col=0)
```


```{r}
decision_tree_report(dt_camp,train,test)

```



### 4. Mixed Model

```{r}
mix_mod<-glm(y~age+job+balance+marital+housing+loan+duration+campaign+day+month+previous,'binomial',data=train)
summary(mix_mod)

```

```{r}   
anova(mix_mod,test='Chisq')

```

```{r}
threshold=0.3
model_report(mix_mod,train,test,threshold)

```

```{r}
roc_auc_curve(mix_mod,test)
```

```{r}
#options(repr.plot.width=6, repr.plot.height=5)

dt_mix<-rpart(y~age+job+balance+marital+housing+loan+duration+campaign+day+month+previous,cp=0.001,maxdepth=7, minbucket=5,method='class',data=train)
prp(dt_mix) #,space=4,split.cex=1.5, nn.border.col=0)
```


```{r}
decision_tree_report(dt_mix,train,test)

```



